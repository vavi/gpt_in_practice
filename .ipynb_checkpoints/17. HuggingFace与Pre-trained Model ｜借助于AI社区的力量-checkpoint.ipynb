{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1840e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = \"gpt4\"\n",
    "model = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240274ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def work_on(input):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=deployment, # 如果直接访问OpenAI GPT服务的同学，这里不要使用engine这个参数，要使用model，如： model=“gpt-4”\n",
    "        model = model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a senior software engineer.\"},   \n",
    "            {\"role\": \"user\", \"content\": input}\n",
    "        ],\n",
    "        temperature = 0.9, \n",
    "        max_tokens = 1000,\n",
    "      )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037eaaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在Kubernetes中，Horizontal Pod Autoscaler(HPA)的主要作用是基于观察到的CPU利用率自动调整副本控制器，部署或复制集的Pod数量。\n",
      "\n",
      "以下是一个简单的HPA配置示例，用于自动缩放名为nginx-deployment的Deployment。在这个例子中，我们的目标是维护CPU利用率在60%，并允许Pod数量在最少2个和最多10个之间变动。\n",
      "\n",
      "```\n",
      "apiVersion: autoscaling/v2beta2\n",
      "kind: HorizontalPodAutoscaler\n",
      "metadata:\n",
      "  name: nginx-deployment-hpa\n",
      "  namespace: default\n",
      "spec:\n",
      "  scaleTargetRef:\n",
      "    apiVersion: apps/v1\n",
      "    kind: Deployment\n",
      "    name: nginx-deployment\n",
      "  minReplicas: 2\n",
      "  maxReplicas: 10\n",
      "  metrics:\n",
      "  - type: Resource\n",
      "    resource:\n",
      "      name: cpu\n",
      "      target:\n",
      "        type: Utilization\n",
      "        averageUtilization: 60\n",
      "```\n",
      "\n",
      "这个配置中的关键部分是：\n",
      "\n",
      "- `kind: HorizontalPodAutoscaler`：这表示我们正在创建一个HorizontalPodAutoscaler资源。\n",
      "\n",
      "- `scaleTargetRef`: 这是我们希望自动缩放的资源的引用。在这个例子中，它是一个名为nginx-deployment的Deployment。\n",
      "\n",
      "- `minReplicas`和`maxReplicas`: 这是允许的Pod数量的上下限。\n",
      "\n",
      "- `type: Resource`：这表示我们要缩放的指标类型是资源类型，也就是它将根据CPU利用率来缩放。\n",
      "\n",
      "- `averageUtilization: 60`：这表示我们的目标CPU利用率是60%。即如果CPU利用率超过60%，将会增加Pod的数量。如果CPU利用率降低到60%以下，将会减少Pod的数量。\n"
     ]
    }
   ],
   "source": [
    "print(work_on(\n",
    "\"\"\"\n",
    "为部署kubernetes nginx-deployment写一个HPA配置\n",
    "集群的pod数在2到10之间，目标CPU 利用率60%\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363437a",
   "metadata": {},
   "source": [
    "# Transformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba8ecdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (4.29.2)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/13/30/54b59e73400df3de506ad8630284e9fd63f4b94f735423d55fc342181037/transformers-4.33.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.33.1-py3-none-any.whl.metadata (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m728.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/34/0e/12d55d5dd648b8f7ea7216c5b7cef9703b4dbd3b2a042872c711d5e98551/safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl.metadata\n",
      "  Downloading safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mbj0593/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.3.3-cp311-cp311-macosx_13_0_arm64.whl (406 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.9/406.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.29.2\n",
      "    Uninstalling transformers-4.29.2:\n",
      "      Successfully uninstalled transformers-4.29.2\n",
      "Successfully installed safetensors-0.3.3 transformers-4.33.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c512593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d44765ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption(input):\n",
    "    output = pipe(input)\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04ddde24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://free-images.com/md/7687/blue_jay_bird_nature.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mbj0593/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "araffy blue jay sitting on a tree stump in the winter\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "image_url = \"https://free-images.com/md/7687/blue_jay_bird_nature.jpg\"\n",
    "#image_url = \"/Users/mbj0593/Downloads/blue_jay_bird_nature.jpg\"\n",
    "display(Image(url=image_url)) \n",
    "print(caption(image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95fbe5",
   "metadata": {},
   "source": [
    "# 看图说话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394f2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def write_a_story(request):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=deployment, # 如果直接访问OpenAI GPT服务的同学，这里不要使用engine这个参数，要使用model，如： model=“gpt-4”\n",
    "        temperature = 0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "              You are a novel writer, please, write a small story (about 200 words) in Chinese according to user input,\n",
    "              \"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": request}\n",
    "        ]\n",
    "      )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9679742b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://free-images.com/md/7687/blue_jay_bird_nature.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在冬季的某一天，一只名叫阿拉菲的蓝松鸦坐在树桩上。它的羽毛在雪白的背景下显得格外醒目，蓝得如同深海一般神秘。阿拉菲看着周围的世界，一片冰冷的寂静。\n",
      "\n",
      "它的眼中反射出周围的雪景，那是一片无尽的白色世界。阿拉菲并不感到孤独，它知道，这就是它的家，这就是它的世界。\n",
      "\n",
      "突然，一阵风吹过，带走了阿拉菲身边的雪花。阿拉菲振动着羽毛，仿佛在向风致敬。然后，它从树桩上飞起，消失在了飘雪的天空中。\n",
      "\n",
      "阿拉菲的故事就这样在冬季的雪景中展开，它的生活充满了冒险和未知。但无论何时，无论何地，阿拉菲都会记得，那个冬季的树桩，那个属于它的家。\n"
     ]
    }
   ],
   "source": [
    "image_url = \"https://free-images.com/md/7687/blue_jay_bird_nature.jpg\"\n",
    "display(Image(url=image_url)) \n",
    "print(write_a_story(caption(image_url)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc9a24",
   "metadata": {},
   "source": [
    "# LangChain + HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bf29ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['HUGGINGFACEHUB_API_TOKEN']='your key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed651c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"bigscience/bloom\", model_kwargs={\"temperature\": 0.5, \"max_length\": 64})\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"hints\"],\n",
    "    template = \"A story about the bird \\n{hints}. \"\n",
    ")\n",
    "chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "print(chain.run(\"araffy blue jay sitting on a tree stump in the winter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e83d42f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
